
#folder and project information

project: amex_kaggle_2022

entity: vivektewari2000
data_loc: /home/pooja/PycharmProjects/amex_default_kaggle/data/
output_loc: /home/pooja/PycharmProjects/amex_default_kaggle/outputs/
weight_loc : /home/pooja/PycharmProjects/amex_default_kaggle/data/weights/
rough_loc: /home/pooja/PycharmProjects/amex_default_kaggle/data/data_created/rough/
group: na
feature_file_name: "feature_importance_v3.xlsx"
## TRAINING params

## MODEL params
model_arch: transformer_encoder_block
model:  simple_nn #var_attention_block #transformer_v1 #transformer_v1   #simple_nn #simple_attention_block #
input_dim: [1,53,12,12]
(self,input_size,output_size,num_heads=4,drop_out=0:
  2):
model_params:
  input_size : 192
  output_size : 1
  num_heads : 4
  drop_out : 0.2



## OPTIMIZER params
optimizer_class: Adam
optimizer_kwargs:
  lr: 5e-5
  # See https://arxiv.org/pdf/2105.05246.pdf
  eps: 0.0003
  #alpha: 0.9
min_lr_mod: 0.01

## LOSS params
loss_func: BCELoss # L2Loss #L2Loss_with_penality  # L2Loss #Weighted_BCELoss #Weighted_BCELoss #distance_BCELoss # BCELoss # Weighted_BCELoss #
# lambda parameter for TD-lambda and UPGO losses
lmb: 0.9
reduction: sum

# MISCELLANEOUS params
learning_rate: 0.01
momentum: 0.01
epsilon: 0.01
alpha: 0.01
actor_device: cuda:1
learner_device: cuda:0
model_log_freq: 100
data_loader: amex_dataset
data_loader_params:
  max_rows: 100000
  max_seq: 13



